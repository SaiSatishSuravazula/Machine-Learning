{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Regression Exercise\n",
        "\n",
        "In this exercise, we will be predicting the median housing price in an area given certain attributes that describe area. We will be working with california housing prices dataset. Each sample in the dataset corresponds to an area in california. The attributes are latitude, longitude, median age of houses in the area (in years), total number of rooms in the area, total number of bed rooms in the area, population of the area, number of households in the areas, median income in the area (in tens of thousands of dollars), the area's proximity to ocean and the median house value."
      ],
      "metadata": {
        "id": "hwMiTLF5kbMp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Get Data"
      ],
      "metadata": {
        "id": "_zSMW8ySoLdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "rVxcQEcgcaTU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70e85476-7272-441c-d4be-96083b05b440"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HOUSING_PATH = \"/content/drive/MyDrive/myLMSCourses/ML/2021_22_evenSemBatch/Practicals/1_exercise1/\"\n",
        "HOUSING_PATH = \"/content/drive/MyDrive/Machine_Learning\" #upload the housing.tgz file into the drive and give the path here\n"
      ],
      "metadata": {
        "id": "pSiMkg4icLqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vzFGqcNI-gf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "\n",
        "def load_housing_data(housing_path=HOUSING_PATH):\n",
        "  tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
        "  housing_tgz = tarfile.open(tgz_path)\n",
        "  housing_tgz.extractall(path=housing_path)\n",
        "  housing_tgz.close()\n",
        "  csv_path = os.path.join(housing_path, \"housing.csv\")\n",
        "  return pd.read_csv(csv_path)\n",
        "\n",
        "housing = load_housing_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Peek into Data"
      ],
      "metadata": {
        "id": "puSCuI7joTka"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look at the top five rows of data using dataframe's head method\n",
        "# Your code below\n",
        "housing.head()"
      ],
      "metadata": {
        "id": "o0uEgK3Mc6SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a quick description of data using dataframe's info method; make a note of attributes with missing values in this notebook below this cell using a markdown cell.\n",
        "# Your code below\n",
        "housing.info()"
      ],
      "metadata": {
        "id": "gIJOwb8-eGu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Attributes with missing values:\n",
        "\n",
        "total_bedrooms"
      ],
      "metadata": {
        "id": "rBETF9xp492m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# What kind of attribute is ocean_proximity? Discrete or continuous? If discrete, use value_counts method on the column corresponding\n",
        "# to ocean_proximity and get a description?\n",
        "# Your code below\n",
        "housing[\"ocean_proximity\"].value_counts()\n"
      ],
      "metadata": {
        "id": "NdIuYGZyfl8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing[\"ocean_proximity\"].describe()"
      ],
      "metadata": {
        "id": "qt7ex24eNGhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ocean_proximity is a categorical attribute.\n",
        "\n",
        "It is discrete."
      ],
      "metadata": {
        "id": "-iAnWYdO54Me"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use describe method on the dataframe to get a summary of the numerical attributes\n",
        "housing.describe()"
      ],
      "metadata": {
        "id": "F_XBbT0_hMhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical attributes can also be described using histograms\n",
        "# Observe how many attributes are thick tailed? What about the scales of the attributes? Are they uniform or vastly different?\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "#use hist method on dataframe\n",
        "# Your code below\n",
        "housing.hist(bins=100, figsize=(20,15))\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FXjOzsiXv6H9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thick-tailed\n",
        "\n",
        "  A thick-tailed attribute is one whose distribution has a higher probability of extreme values than a normal distribution.\n",
        "\n",
        "  Thick-tailed attributes: latitude, housing_median_age, median_house_value\n",
        "\n",
        " Scales of the attributes are vastly different\n"
      ],
      "metadata": {
        "id": "X7Qa0nopBgYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Test Data\n",
        "Why create test set now, right at the beginning? The reason is to avoid data snooping bias. That is, the more we look into data, our brain is powerful to capture the pattern seen in that data and will naturally influence our choice of the model. But that model may not generalize well during deployment since it was chosen simply based on some pattern seen in some sample data. So, it is better to separate out test set right in the beginning and keep it only for testing. We can do a pure random split of data into train and test set. For this you can look at train_test_split class in sklearn.model_selection. But, suppose your manager told that median income is an important attribute for predicting house price. Then, you would want to split data so that it reflects the various categories of median income in both train and test data. This is called as stratified sampling. Of course, median income is right now a numeric attribute. You have to create a new categorical attribute called income_cat, use that to split data into train and test sets, and then remove the income_cat attribute. To create income_cat, we can look at its respective histogram above and find that most median incomes are clustered around 1.5-6 (i.e \\$15000-\\$60000). So our categories could be 0-1.5, 1.5-3, 3-4.5, 4.5-6, >6."
      ],
      "metadata": {
        "id": "26fmg5INohfa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# create income_cat attribute as described above\n",
        "# housing[\"income_cat\"] = # fill the code here; use cut method in pandas\n",
        "\n",
        "categories = [0,1.5,3,4.5,6,float('inf')]\n",
        "titles = [\"very low\", \"low\", \"average\", \"high\", \"very high\"]\n",
        "housing[\"income_cat\"] = pd.cut(housing.median_income, bins = categories, labels = titles, include_lowest = True)\n"
      ],
      "metadata": {
        "id": "de0e10_qhpb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "housing.head()"
      ],
      "metadata": {
        "id": "d4tTTN6OJmjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train test stratified split (80-20 split) using income_cat attribute; use the train_test_split() method in sklearn.model_selection module\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Your code below\n",
        "X = housing\n",
        "y = housing[\"median_house_value\"]\n",
        "income_cat = housing[\"income_cat\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=income_cat)\n"
      ],
      "metadata": {
        "id": "WgPDBQi38BX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to check if the stratified split worked, compute and display the proportions of income categories in the test set and the whole dataset, and compare\n",
        "# and make your observations in a markdown cell below.\n",
        "\n",
        "# Your code below\n",
        "# print(X_test[\"income_cat\"].value_counts()/len(X_test))\n",
        "# print(housing[\"income_cat\"].value_counts()/len(housing))\n",
        "\n",
        "# ----------------------------------------------------------\n",
        "\n",
        "# Compute proportions of income categories in the whole dataset\n",
        "income_cat_proportions = income_cat.value_counts(normalize = True)\n",
        "\n",
        "# Compute proportions of income categories in the test set\n",
        "test_set_proportions = X_test[\"income_cat\"].value_counts(normalize = True)\n",
        "\n",
        "# Print the proportions\n",
        "print(income_cat_proportions)\n",
        "print(test_set_proportions)\n"
      ],
      "metadata": {
        "id": "z4b9wOL93tVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Good Stratified split\n",
        "\n",
        "The distribution of values of income_cat attribute in the whole data set and the test is preserved. Therefore this is a good stratified split."
      ],
      "metadata": {
        "id": "lVJH9BxQXjqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# drop the income_cat attribute from both train and test set; use dataframes's drop method\n",
        "# Your code below\n",
        "\n",
        "X_train = X_train.drop(\"income_cat\", axis=1)\n",
        "X_test = X_test.drop(\"income_cat\", axis=1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0qBXOEV9-Ugj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "GfU6MWTvRK5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Data\n",
        "We will explore train data more to gain more insights. We will not touch test data. It will only be used at the end when we build a model and we are ready to test it. Even for exploring train set, to be on the safer side, we will make a copy of it. We will first visualize train data using scatter plot. See the plot below carefully. A lot of information has been embedded into it. Answer the questions given in comments."
      ],
      "metadata": {
        "id": "oEYcFHNtDpI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing = X_train.copy()\n",
        "\n",
        "housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.8, s=housing[\"population\"]/100,\n",
        "         label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
        "         colorbar=True)\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "# What does the size of the scatter point indicate? What does the color of the scatter point indicate? What is the relationship\n",
        "# between ocean_proximity and median_house_value (note that the ocean is towards the bottom left in the plot)? Is there relationship between population and median_house_value?\n",
        "# Describe the role of alpha parameter in the dataframe's plot method."
      ],
      "metadata": {
        "id": "akZeKFir-4dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Observations.\n",
        "\n",
        "\n",
        "*   The bigger the size of scatter point, the more the\n",
        " population\n",
        "is residing at that particular location of scatter point.\n",
        "*   The color of scatter point indicates the median_house_value whose color code is given by the colorbar.\n",
        "*   Most of the houses that are near the water bodies have high median_house_value.\n",
        "*   There is no significant relationship between population and median_house_value.\n",
        "*  Alpha is a parameter of transparency\n",
        "\n",
        "   0-fully transparent\n",
        "\n",
        "   1-fully opaque\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XHv5x5WG8iaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now look at linear correlations between median_house_value and all other attributes. Complete the code below  and answer the questions given in comments. In case you are not familiar with the concept of Perason's correlation, read about it."
      ],
      "metadata": {
        "id": "nW7XMDlyIm3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use dataframe's corr method to get correlation matrix of every pair of attributes\n",
        "# fill code here\n",
        "corr_matrix = housing.corr(numeric_only=True)  # Add numeric_only=True to handle non-numeric columns\n",
        "corr_matrix\n",
        "\n",
        "\n",
        "# extract only median_house_value column from corr_matrix and sort it in descending order\n",
        "# for sorting, use pandas series method sort_values\n",
        "# Your code below\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
        "\n",
        "# Which attribute correlates positively highly with median_house_value? Are there attributes which have negligible linear correlations\n",
        "# with median_house_value? What about negative linear correlations? Does a correlation value of zero or close to zero mean absolutely no relationship?"
      ],
      "metadata": {
        "id": "O8Rm8uozF_7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "median_income correlates positively highly with median _house_value.\n",
        "\n",
        "Yes, there are attributes that have negligible linear correlation with median_house_value. Some of them are the population of the block, number of households in the block, longitude, and total_bedrooms.\n",
        "\n",
        "No it does not mean there is no relationship at all. There can be some non-linear relationship.\n",
        "\n"
      ],
      "metadata": {
        "id": "2p_ZRIt0IdCg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # Since median_income highly correlates with median_house_value, let's focus on that.\n",
        " # Display a scatter plot of median_income vs median_house_value\n",
        " # Your code below\n",
        "housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\", alpha=0.5, s=housing[\"population\"]/100,\n",
        "        label=\"population\", figsize=(10,7), cmap=plt.get_cmap(\"jet\"),)\n",
        "\n",
        "#  c=\"population\", colorbar=True\n",
        "# housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.8, s=housing[\"population\"]/100,\n",
        "#          label=\"population\", figsize=(10,7), c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),\n",
        "#          colorbar=True)\n",
        "# Does the plot reveal anything? Change alpha value and see."
      ],
      "metadata": {
        "id": "ae6YVvcpKxAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The below code is just for understanding the data\n",
        "\n",
        "housing[(housing[\"median_income\"] >= 1) & (housing[\"median_income\"] <= 6) & (housing[\"median_house_value\"] > 480000)][\"ocean_proximity\"].value_counts()"
      ],
      "metadata": {
        "id": "zYolnP0vPKkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Observations.\n",
        "\n",
        "median_income - Median income for households within a block (measured in ten thousand US Dollars.)\n",
        "\n",
        "\n",
        "medianHouseValue -\tMedian house value for households within a block (measured in US Dollars)\n",
        "\n",
        "\n",
        "We can make out a linear relation between median_income and median_house_value. The reason why even less median_income population has more median_house_value is because of the ocean_proximity factor that we didn't include in our correlation matrix."
      ],
      "metadata": {
        "id": "0WKsFFFONZ8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Attribute like total_rooms, total_bedrooms, population are too general to relate to house price\n",
        "# Above correlations also show this\n",
        "# Why not create population per household, rooms per household, ratio of bed_rooms to rooms?\n",
        "housing[\"rooms_per_household\"] = housing[\"total_rooms\"]/housing[\"households\"]\n",
        "housing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]/housing[\"total_rooms\"] # Fill your code here\n",
        "housing[\"population_per_household\"] = housing[\"population\"]/housing[\"households\"] # Fill your code here\n"
      ],
      "metadata": {
        "id": "qMnn9zpxNcMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now see the correlations of median_house_value to all the attributes including new attributes introduced above\n",
        "# Your code below\n",
        "corr_matrix = housing.corr(numeric_only=True)\n",
        "corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
        "\n",
        "# Any new observations ??\n"
      ],
      "metadata": {
        "id": "Qv9NVm9rO-mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observations.\n",
        "rooms_per_household has a better positive correlation value than total_rooms with the median_house_value."
      ],
      "metadata": {
        "id": "_lpIwHGTgFIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare (Preprocess) Data\n",
        "We have explored train data and found that some derived attributes may be useful. Now, we will need to preprocess and prepare the data before building the model to process the prepared data. For the data under consideration we need to do the following:\n",
        " 1. Separate the label (median_house_value) and the rest of the attributes\n",
        " 2. Fill the missing values in total_bedrooms attribute with median of rest of of the entries in it.\n",
        " 3. As we noted earlier, scales of attributes are vastly different. Bring all of them to uniform scale using standardization. That is, for each attribute (not the label), subtract mean of it from each of its entry and divide by its standard deviation. This way, all attributes will become zero centred and will have its scale in standard deviation units. There is another way called as normalization to bring all attributes into uniform scale. Here, for each attribute, subtract min of that attribute from each of its entry and divide by max minus min of that attribute. This will ensure the range of attribute is in [0, 1]. Standardization is preferred over normalization generally since normalization are more sensitive to outliers than standardization. For eg, guess what will happen if one attribute had all the values in the range 0-15 except for one (which is 100). In normalization, the range will simply get crushed to [0, 0.15] whereas in standardization it is more likely to be wider.\n",
        " 4. Note that ocean_proximity is a categorical attribute. We need to convert it to numerical attribute before building the model. One way of doing this is to simply assign 0, 1, 2... to the categories. This might work in certain situations but not always. For eg, in the situation here, assigning 0 to <1H OCEAN, 1 to INLAND, 2 to NEAR OCEAN, 3 to NEAR BAY and 4 to ISLAND would mean to the model that NEAR OCEAN and INLAND are closer than NEAR OCEAN and <1H OCEAN, which is wrong. Instead, since there are 5 categories, we will represent each category by a binary vector of length 5 such that one unique component of this binary vector is 1 and rest are zero for this category, and so on. In other words, <1H OCEAN will be represented by 10000, INLAND will be represented by 01000, and so on. This is called as one hot encoding. We will be converting ocean_proximity into one hot encoding representation. One hot encoding will not be efficient if the number of catgories are very large. For eg, imagine that the categorical attribute is vocabulary and the number of categories are number of words in the vocabulary. But in the case here, we have only 5 categories. So, no problem with one hot encoding."
      ],
      "metadata": {
        "id": "6C2MzUOvLcpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First separate the label median_house_value\n",
        "# Fill your code here; make a copy of the median_house_value\n",
        "# drop the median_house_value from housing dataframe inplace\n",
        "# Your code below\n",
        "housing_labels = housing[\"median_house_value\"].copy()\n",
        "housing.drop(\"median_house_value\", axis=1, inplace = True)\n",
        "\n",
        "# Let me save my housing_labels as csv file in my G_drive at mentioned path.\n",
        "housing_labels_df = pd.DataFrame(housing_labels)\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/Machine_Learning/housing_labels_train.csv'  # Replace with your desired path\n",
        "\n",
        "# Save the file\n",
        "housing_labels_df.to_csv(file_path, index=False, header=['median_house_value'])  # Set header to column name\n"
      ],
      "metadata": {
        "id": "W7pc4nszLcpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill any missing value with median value of the attribute it corresponds to\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "imputer = SimpleImputer(strategy = \"median\")  # Fill your code here; instantiate SimpleImputer object with median strategy\n",
        "\n",
        "# Note that the imputer will be automatically used on numerical attributes once we set up the pipeline.\n",
        "# Right now, we have instantiated it."
      ],
      "metadata": {
        "id": "nY7nQ3Vb3dW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "std_scaler = StandardScaler()# Fill your code here; instantiate StandardScaler object\n"
      ],
      "metadata": {
        "id": "vMOcZWXN_zZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the pipeline consisting of above two transforms which deal with numerical attributes\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "#Fill your code here; instantiate Pipeline object with the above two transforms in order\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', imputer),\n",
        "    ('std_scaler', std_scaler)\n",
        "])\n",
        "\n",
        "\n",
        "# The way this pipeline works is as follows. When we call its fit_transform method on the data devoid of categorical attributes,\n",
        "# it will first call the fit_transform method of the imputer object. The fit_transform method of the imputer object will first call\n",
        "# fit method of the imputer object which will compute the median of all numerical attributes, respectively (based on their non-null values)\n",
        "# and store it in statistics_ public variable and return itself. Then the transform method from SimpleImputer class is called on the returned\n",
        "# imputer object which will fill the missing values with the respective median value picked up from statistics_ variable.\n",
        "# The transform method returns the transformed data. This will then go as input to the fit method of the std_scaler transform in the\n",
        "# pipeline. The fit method will compute mean and std dev with respect to each attribute and store them in mean_ and scale_ public variables,\n",
        "# and return the std_scaler object itself. Then the transform method from the StandardScaler class is called on the returned std_scaler\n",
        "# object which will do the standardization on each numerical attribute, respectively. The transformed data will be returned by the pipeline.\n",
        "\n",
        "# Currently we are not calling the fit_tranform method on the pipeline because that will require data to be devoid of categorical\n",
        "# attributes. As already explained earlier, we will not do that. Instead we will set up another pipeline which will handle\n",
        "# this pipeline and the transforms on categorical attributes automatically.\n",
        "\n"
      ],
      "metadata": {
        "id": "RpluggLJAr1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# represent ocen_proximity attribute in one hot vector encoding\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "categorical_encoder = OneHotEncoder()# Fill your code here; instantiate OneHotEncoder object\n"
      ],
      "metadata": {
        "id": "XYkdX5B-CBd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up the ColumnTransformer pipeline that will automatically deal with both numerical pipeline and OneHotEncoder transform.\n",
        "from sklearn.compose import ColumnTransformer\n",
        "        # Fill your code here; get the list of numerical attribute names - first get all attribute names; then remove\n",
        "                     # ocean proximity; Note that remove method of list does inplace removal and returns none if success; if you are\n",
        "                     # using remove method, don't chain it with list creation; do it in the next line separately.\n",
        "mylist = list(housing.columns)\n",
        "mylist.remove(\"ocean_proximity\")\n",
        "numerical_attribs =  mylist\n",
        "print(mylist)\n",
        "\n",
        "# Fill your code here; create a list with one entry which is ocean_proximity\n",
        "\n",
        "categorical_attribs = [\"ocean_proximity\"]\n",
        "\n",
        "# full_pipeline = # Fill your code here; instantiate ColumnTransformer pipleline with numerical pipeline followed by one hot encoder\n",
        "full_pipeline =  ColumnTransformer([(\"numerical\", numerical_pipeline, numerical_attribs),\n",
        "                                      (\"categorical\", categorical_encoder, categorical_attribs)\n",
        "  ])\n",
        "\n",
        "\n",
        "# housing_prepared = # Fill your code here; call the fit_transform method on the ColumnTransformer pipeline object\n",
        "housing_prepared = full_pipeline.fit_transform(housing)\n",
        "\n",
        "# housing_prepared is normalized X_train without \"median_house_value\" and more columns have been added\n",
        "\n",
        "# Get the feature names after transformation\n",
        "feature_names = numerical_attribs.copy()\n",
        "feature_names.extend(full_pipeline.named_transformers_['categorical'].get_feature_names_out())\n",
        "\n",
        "# Save to CSV with updated header\n",
        "housing_prepared_df.to_csv(file_path, index=False, header=feature_names)\n",
        "\n",
        "# Let me save housing_prepared as csv file in my Google Drive at specified path\n",
        "housing_prepared_df = pd.DataFrame(housing_prepared)\n",
        "\n",
        "# Specify the path in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/Machine_Learning/housing_prepared_train.csv'\n",
        "\n",
        "# Save to CSV\n",
        "housing_prepared_df.to_csv(file_path, index=False, header = feature_names)\n",
        "\n",
        "# The pipeline works similar to what was already explained earlier.\n",
        "housing_prepared.shape\n"
      ],
      "metadata": {
        "id": "CT-cYnjzIJp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select and Train Model\n",
        "Now that the data is prepared, we need to select a model, train it and see how it performs. Sometimes application and data can guide towards model selection. For example, if the volume of data is very large and the application is computer vision or natural language processing, neural networks are very powerful models. As one gains more experience, it becomes relatively easy to identify the family of models that would suit a particular problem at hand. However, one may not be able to nail down to the best model right at the first instance, even with experience. It is always an iterative process. Multiple models have to compared before finalizing on the model to be deployed. For now, we will only train a linear regression model in this notes."
      ],
      "metadata": {
        "id": "nV7n1QT8vcn0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "# Fill your code here; instantiate LinearRegression object\n",
        "\n",
        "\n",
        "# Now fit the data using fit the method on linear regression object\n",
        "# Your code below\n",
        "lin_reg.fit(housing_prepared, housing_labels)"
      ],
      "metadata": {
        "id": "yD9TyQJ1Ln4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's try out this model on few instances from train data\n",
        "\n",
        "sample_data = housing.head()# Fill your code here; get first five rows from housing dataframe\n",
        "# sample_labels = # Fill your code here; get first five labels from housing_labels pandas series\n",
        "sample_labels =  housing_labels.head()\n",
        "# sample_data_prepared = # Fill your code here; transform the sample_data using full_pipeline.\n",
        "                        # Note that you need to only transform this data; not fit. Fitting was already done.\n",
        "sample_data_prepared = full_pipeline.transform(sample_data)\n",
        "\n",
        "# predictions = # Fill your code here; get the predictions on sample_data using predict method on the already fitted linear regression object\n",
        "predictions = lin_reg.predict(sample_data_prepared)\n",
        "print(\"Predictions: \", predictions)\n",
        "print(\"Groundtruth: \", list(sample_labels))\n"
      ],
      "metadata": {
        "id": "iWAz_YWUxTTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the predictions are not very close. Infact they are off by around 27% on average. We can check the performance on the entire training set. Before doing that, we need a way of quantifying the performance. There are three standard performance measure for linear regression viz. mean absolute error (MAE), mean squared error (MSE) and root mean squared error (RMSE).\n",
        "\n",
        "MAE = $\\frac{1}{n}\\sum_{i=1}^{n} |(y^i-\\hat y^i)|$\n",
        "\n",
        "MSE = $\\frac{1}{n}\\sum_{i=1}^{n} (y^i-\\hat y^i)^2$\n",
        "\n",
        "RMSE = $\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (y^i-\\hat y^i)^2}$\n",
        "\n",
        "The linear regression model can be fit by minimizing any of the above errors. Note that MSE punishes large errors more severely than MAE. So, MSE is sensitive to outliers than MAE. Further, minimizing MSE results in units of the response getting squared and so intrepretation becomes difficult. Hence, RMSE is preferred over MSE which maintains MSE. We will use RMSE below to see the performance, but we can try with any of the above."
      ],
      "metadata": {
        "id": "gaz1YpC_2Y8w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "housing_labels.describe()"
      ],
      "metadata": {
        "id": "qfAz8h90StPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "predictions = lin_reg.predict(housing_prepared)\n",
        "lin_mse = mean_squared_error(housing_labels, predictions)\n",
        "lin_rmse = np.sqrt(lin_mse)\n",
        "print(f\"RMSE: {lin_rmse:.2f}\")"
      ],
      "metadata": {
        "id": "1NZgyy60z9bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this means is that the predictions are off by \\$67,629 on the training set. Is this a good performance on the train set? Compare this offset with the range of median_house_value between  $25^{th}$ and $75^{th}$ percentile we had obtained earlier using dataframe's describe method. In fact the model is underfitting the data. We need to look for more complex models, like decision tree or random forest.\n"
      ],
      "metadata": {
        "id": "Mek0iYEK9mim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Observation\n",
        "\n",
        "The range of median_house_value between  25th  and  75th  percentile we had obtained earlier using dataframe's describe method is 145725\n",
        "\n",
        "The above model's RMSE is substantial (almost 45%) when compared to middle 50% the house prices (varying from around 1 lakh to around 2.5 lakhs). So it did not perform very well on training set.\n",
        "\n"
      ],
      "metadata": {
        "id": "Hbn0mnYNRpBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tune Model\n",
        "We saw that the model was underfitting. In some cases, the model may overfit (i.e give 0 train error). We know that we should look for better models. But, since we have now studied only regression, we need to wait until we pick other models. In any case, in the end-end ML solution, once we fit the model, we try to finetune its hyperparameters using a validation dataset. That is, we try different values of hyperparameters for the model, check its performance on the validation set and choose the one which gives the best performance. The validation dataset should neither intersect with train nor the test set. If it intersects with trainset, the model is likely to overfit depending on the amount of intersection. And we know that the test set is not to be touched at all during training. In linear regression, the only hyper parameter we can think of tuning is whether to have the intercept parmeter or not. However, in the example we are studying here, the performance is not going to change (check for yourself). So, we will not not be doing any hyperparameter finetuning here. But remember that this step is important in the end-end ML solution. We will understand more about hyperparameters as we go along.\n",
        "\n",
        "Of course, we can also a lot of analysis like what we had one in our earlier linear regression notes to finetune the model. I leave that as optional exercise."
      ],
      "metadata": {
        "id": "GdpJpdKtB1wI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on Test Set\n",
        "Assume that the linear regression model is the best model we have got after all the above steps. Now, we will test the model on the test set."
      ],
      "metadata": {
        "id": "cYt9xltAFVym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_drop = X_test.drop(\"median_house_value\", axis = 1)# Fill your code here; drop median_house_value from strat_test_set\n",
        "test_labels = X_test[\"median_house_value\"].copy() # Fill your code here; get a copy of median_house_value column from strat_test_set\n",
        "\n",
        "\n",
        "housing_labels_test_df = pd.DataFrame(test_labels)\n",
        "\n",
        "# Specify file path\n",
        "file_path = '/content/drive/My Drive/Machine_Learning/housing_labels_test.csv'  # Replace with your desired path\n",
        "\n",
        "# Save the file\n",
        "housing_labels_test_df.to_csv(file_path, index=False, header=['median_house_value'])  # Set header to column name\n",
        "\n",
        "# add the derived attributes rooms_per_household, bedrooms_per_room, population_per_household\n",
        "# Your code below\n",
        "X_test_drop[\"rooms_per_household\"] = X_test_drop[\"total_rooms\"] / X_test_drop[\"households\"]\n",
        "X_test_drop[\"bedrooms_per_room\"] = X_test_drop[\"total_bedrooms\"] / X_test_drop[\"total_rooms\"]\n",
        "X_test_drop[\"population_per_household\"] = X_test_drop[\"population\"] / X_test_drop[\"households\"]\n",
        "\n",
        "X_test_prepared = full_pipeline.transform(X_test_drop)# Fill your code here; transform X_test through full pipeline\n",
        "\n",
        "# Let me save housing_prepared as csv file in my Google Drive at specified path\n",
        "housing_prepared_test_df = pd.DataFrame(X_test_prepared)\n",
        "\n",
        "# Specify the path in your Google Drive\n",
        "file_path = '/content/drive/MyDrive/Machine_Learning/housing_prepared_test.csv'\n",
        "\n",
        "# Save to CSV\n",
        "housing_prepared_test_df.to_csv(file_path, index=False, header = feature_names)\n",
        "\n",
        "test_predictions = lin_reg.predict(X_test_prepared)# Fill your code here; get test predictions\n",
        "test_mse = mean_squared_error(test_labels, test_predictions)# Fill your code here; compute test mse\n",
        "test_rmse = np.sqrt(test_mse)# Fill your code here; compute test rmse\n",
        "\n",
        "print(f\"RMSE: {test_rmse:.2f}\")"
      ],
      "metadata": {
        "id": "py1oBRUb9Xyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-launch\n",
        "Before you launch the model, you need to present it to your team to get their final approval. So, as a pre-launch exercise, you will submit the completed assignment to me."
      ],
      "metadata": {
        "id": "DI9U8sbeJO_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Launch\n",
        "You can launch your model as webservice through a REST API or through some other means. We will not dicuss that now."
      ],
      "metadata": {
        "id": "2yz3bUKqKEex"
      }
    }
  ]
}